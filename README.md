# ğŸ§  Intelligent Document Q&A with LangChain, FAISS & HuggingFace

> Ask questions from your PDF, DOCX, or TXT files using powerful open-source tools â€” all in Google Colab.

---

## ğŸ“Œ Overview

This project allows you to upload a document and ask it natural language questions. Behind the scenes, it uses:

* ğŸ§© **LangChain** â€“ to handle document loading, text splitting, and chaining logic
* ğŸ” **FAISS** â€“ to store and retrieve semantically similar chunks
* ğŸ¤— **HuggingFace Transformers** â€“ to run a local language model for question-answering
* âš¡ **Google Colab** â€“ no setup required, runs in the cloud

---

## ğŸš€ Features

 âœ… Upload **PDF**, **DOCX**, or **TXT**  
 âœ… Extract and chunk document contents  
 âœ… Embed chunks using **sentenceâ€‘transformers**  
 âœ… Store & search in a **FAISS** vector index  
 âœ… Use a small, efficient language model (**Flanâ€‘T5**) for answers  
 âœ… Interactive Q&A loop  

---

## ğŸ› ï¸ Installation (Google Colab)

```python
!pip install langchain langchain-community
!pip install faiss-cpu
!pip install pypdf python-docx
!pip install sentence-transformers
!pip install transformers
```

---

## ğŸ“¤ Upload Your Document

```python
from google.colab import files

uploaded = files.upload()
file_path = list(uploaded.keys())[0]
print("Uploaded:", file_path)
```

---

## ğŸ“„ Load & Split the Document

```python
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Choose loader based on file type
if file_path.endswith(".pdf"):
    loader = PyPDFLoader(file_path)
elif file_path.endswith(".docx"):
    loader = Docx2txtLoader(file_path)
else:
    loader = TextLoader(file_path)

docs = loader.load()

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
documents = splitter.split_documents(docs)

print(f"Total Chunks: {len(documents)}")
```

---

## ğŸ§  Generate Embeddings & Build Vector Store

```python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(documents, embeddings)
```

---

## ğŸ¤– Load Local Language Model

```python
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

flan_pipeline = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",  # Swap to flan-t5-small for faster inference
    max_length=512
)

llm = HuggingFacePipeline(pipeline=flan_pipeline)
```

---

## ğŸ”— Set Up QA Chain

```python
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    chain_type="stuff"
)
```

---

## â“ Ask Questions

### One-Off Query

```python
query = "Give me a short summary of the document"
print(qa.run(query))
```

### Interactive Q&A Loop

```python
while True:
    q = input("Ask a question (or 'exit'): ")
    if q.lower() == "exit":
        break
    print("Answer:", qa.run(q))
```

---

## ğŸ§ª Sample Output

```
Ask a question (or 'exit'): what is the importance of a healthy diet?
Answer: helps to protect against malnutrition in all its forms, as well as noncommunicable diseases (NCDs) such as diabetes, heart disease, stroke and cancer
```

---

## ğŸ“ File Types Supported

* `.pdf` â€“ parsed using `PyPDFLoader`
* `.docx` â€“ parsed using `Docx2txtLoader`
* `.txt` â€“ parsed using `TextLoader`

---
